<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Document</title>
    <script src="https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-M/socket.io/4.4.1/socket.io.min.js"></script>
    <script src="../static/js/vudio.js"></script>
    <script src="../static/js/recorder.js"></script>
    <style>

    </style>
</head>

<body>
    <!-- 音频波形可视化 -->
    <canvas id="waveShape"></canvas>

    <!-- 麦克风按钮 -->
    <div id="microphone" style="display: none;"></div>

    <!-- 音频播放容器 -->
    <audio id="audioPlayer" controls autoplay style="display: none"></audio>

    <!-- 添加录音状态显示 -->
    <div id="recordingStatus"
        style="position: fixed; top: 20px; right: 20px; padding: 10px; background-color: #f0f0f0; border-radius: 5px;">
        录音信息
    </div>

    <!-- 添加音频播放状态显示 -->
    <div id="audioPlayingStatus"
        style="position: fixed; top: 80px; right: 20px; padding: 10px; background-color: #f0f0f0; border-radius: 5px;">
        音频播放信息
    </div>

    <!-- 录音结果显示 -->
    <div id="recordingResult">
        录音结果...
    </div>
</body>
<script>
    /**
     * @function initAudioAnalyser
     * @description 初始化音频分析器
     * @param {MediaStream} stream 音频流
     * @returns {Object} 音频分析器和数据数组
     */
    async function initAudioAnalyser(stream) {
        const audioContext = new AudioContext();
        const analyser = audioContext.createAnalyser();
        const microphone = audioContext.createMediaStreamSource(stream);
        microphone.connect(analyser);
        analyser.fftSize = 2048;
        const bufferLength = analyser.frequencyBinCount;
        const dataArray = new Float32Array(bufferLength);

        return {
            analyser,
            dataArray
        };
    }

    // 静音阈值(单位:分贝)
    const SILENCE_THRESHOLD = -30;

    /**
     * @function detectSilence
     * @description 检测用户是否已经停止讲话
     * @param {AnalyserNode} analyser 音频分析器
     * @param {Float32Array} dataArray 数据数组
     * @returns {Boolean} 用户是否已经停止讲话
     */
    function detectSilence(analyser, dataArray) {
        analyser.getFloatTimeDomainData(dataArray);
        let sum = 0;
        for (let i = 0; i < dataArray.length; i++) {
            sum += Math.abs(dataArray[i]);
        }
        const average = sum / dataArray.length;
        const db = 20 * Math.log10(average);
        return db < SILENCE_THRESHOLD;
    }

    window.onload = async () => {
        // 获取状态显示元素
        const statusElement = document.getElementById('recordingStatus');

        // 获取音频流
        let audioStream = await navigator.mediaDevices.getUserMedia({ audio: true, video: false });
        console.log("[test.html][window.onload] 创建 getUserMedia 音频流成功...");

        // 初始化音频分析器
        const { analyser, dataArray } = await initAudioAnalyser(audioStream);

        // 创建媒体录制器
        let mediaRecorder = new MediaRecorder(audioStream);

        // 音频数据列表
        let audioChunks = [];

        // 静音定时器
        let silenceTimer;

        // 静音持续时间阈值（单位：毫秒）
        const SILENCE_DURATION = 2000;

        // 是否录制完毕
        let recordingFinished = false;

        // 检测用户是否停止讲话
        function checkSilence() {
            // console.log('[phone.js][checkSilence] 检测用户讲话事件...');

            // 如果用户停止讲话，则设置短暂的静音等待
            if (detectSilence(analyser, dataArray)) {

                // 如果静音定时器不存在，则设置静音定时器
                if (!silenceTimer) {
                    silenceTimer = setTimeout(() => {
                        stopRecording();

                        // 创建 WAV blob 并传递给 createDownloadLink
                        rec.exportWAV(upload_audio);

                        // 是否录制完毕
                        recordingFinished = true;

                    }, SILENCE_DURATION);
                }
                if (statusElement.textContent != '录音信息：等待说话...') {
                    console.log('[test.html][checkSilence] 用户停止讲话...');
                    statusElement.textContent = '录音信息：等待说话...';
                    statusElement.style.backgroundColor = '#f0f0f0';
                }
            }
            // 如果用户正在讲话，则设置录音状态
            else {
                if (silenceTimer) {
                    clearTimeout(silenceTimer);
                    silenceTimer = null;
                }
                if (!recordingFinished) {
                    continueRecording();
                } else {
                    startRecording();
                    recordingFinished = false;
                }
                if (statusElement.textContent != '录音信息：正在讲话...') {
                    console.log('[test.html][checkSilence] 用户正在讲话...');
                    statusElement.textContent = '录音信息：正在讲话...';
                    statusElement.style.backgroundColor = '#FFB6C1';
                }
            }
        }

        // 使用 setInterval，每 333ms 检查一次
        setInterval(checkSilence, 333);

        // ----- 音频波形可视化 start -----
        const waveShape = document.querySelector('#waveShape');
        let vudio;
        vudio = new window.Vudio(audioStream, waveShape, {
            effect: 'waveform',
            accuracy: 16,
            width: window.innerWidth * 100 / 412 * 1.5,
            height: window.innerWidth * 100 / 412 * 1,
            waveform: {
                maxHeight: 80,
                minHeight: 0,
                spacing: 5,
                color: '#000',
                shadowBlur: 0,
                shadowColor: '#f00',
                fadeSide: true,
                horizontalAlign: 'center',
                verticalAlign: 'middle',
                radius: 20
            }
        });
        vudio.dance();
        // ----- 音频波形可视化 end -----


        /* 处理音频录制 start 
        ------------------------------------------------------------*/

        function startRecording() {

            // 创建新的音频上下文，这是Web Audio API的核心对象
            audioContext = new AudioContext();

            // 将麦克风的音频流(stream)转换为音频源节点
            input = audioContext.createMediaStreamSource(audioStream);

            // 创建一个新的 Recorder 实例，用于录制音频
            // numChannels: 1 表示使用单声道录音，用于减少文件大小
            rec = new Recorder(input, { numChannels: 1 })

            // 启动录制过程
            rec.record()

            // console.log("[voice.js][startRecording] Recording start.");
        }

        function continueRecording() {

            // 启动录制过程
            rec.record()

        }

        function stopRecording() {
            console.log("[test.html][stopRecording] 停止录音...");
            // 告诉录制器停止录制
            rec.stop();

            // 停止麦克风访问
            // audioStream.getAudioTracks()[0].stop();

            // console.log("[voice.js][stopRecording] Recording stop.");
        }


        function upload_audio(blob) {
            var xhr = new XMLHttpRequest();
            xhr.onload = function (e) {
                if (this.readyState === 4) {
                    console.log("[test.html][upload_audio] response:", e.target.responseText);
                }
            };
            var fd = new FormData();
            fd.append("audio_data", blob, "recorded_audio.wav");
            var sampleRate = audioContext.sampleRate;
            fd.append("sample_rate", sampleRate);
            const token = localStorage.getItem('token');
            console.log("[test.html][upload_audio] 上传音频数据...");
            xhr.open("POST", "/agent/upload_audio", true);
            xhr.setRequestHeader('Authorization', 'Bearer ' + token);
            xhr.send(fd);
        }
        /* 处理音频录制 end 
        ------------------------------------------------------------*/

        // 开始录音
        statusElement.textContent = '录音信息：开始录音...';
        // mediaRecorder.start();
        startRecording();
    }


    /* 处理音频播放 start 
    ------------------------------------------------------------*/

    // 创建 socket 连接
    const socket = io();

    // 保存音频数据的列表
    var audioList = [];

    // 当前播放的音频索引
    var audioIndex = 0;

    // 获取音频播放器元素
    const audioPlayer = document.getElementById('audioPlayer');

    // 获取音频播放状态显示元素
    const audioPlayingStatus = document.getElementById('audioPlayingStatus');

    // 播放下一个音频
    function playNextAudio() {
        if (audioList[audioIndex] != undefined) {
            pauseDiv.style.backgroundImage = `url('${'./static/images/pause.png'}')`;
            var audioBlob = new Blob([audioList[audioIndex]], { type: 'audio/mp3' });
            var audioURL = URL.createObjectURL(audioBlob);

            audioPlayer.src = audioURL;

            try {
                audioPlayingStatus.textContent = '音频播放信息：音频片段开始播放...';
                console.log('音频播放信息：音频片段开始播放...');
                audioPlayer.play();
            } catch (error) {
                audioPlayingStatus.textContent = '音频播放信息：音频片段播放失败.';
                console.log('音频播放信息：音频片段播放失败.');
                console.log('错误信息：', error);
            }
        } else {
            console.log('undefined audioIndex: %d', audioIndex);
        }
    }

    // 监听音频播放结束事件
    audioPlayer.onended = function () {
        audioIndex++;
        pauseDiv.style.backgroundImage = `url('${'./static/images/pause_inactive.png'}')`;
        playNextAudio();
    };

    // 监听后端发送的 agent_play_audio_chunk 事件
    socket.on('agent_play_audio_chunk', function (data) {
        var index = data['index'];
        var audioData = data['audio_chunk']; // 后端发送的音频数据

        // 将音频数据添加到音频列表
        audioList[index] = audioData;

        // 如果当前没有音频正在播放，开始播放
        if (audioPlayer.paused) {
            audioPlayer.pause();
            playNextAudio();
        }
    });

    // 获取录音结果显示元素
    const recordingResult = document.getElementById('recordingResult');

    // 监听后端发送的 agent_speech_rec 语音识别事件
    socket.on('agent_speech_rec', function (data) {
        var rec_result = data['rec_result'];
        console.log('[audio.js][socket.on][agent_speech_rec] rec_result: %s', rec_result);
        recordingResult.textContent = rec_result;
    })
    /* 处理音频播放 end
    ------------------------------------------------------------*/
</script>

</html>